{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "file: \n",
    "\tNN.py\n",
    "description:\n",
    "\tA python program that implement a simple Neural Network. \n",
    "author:\n",
    "\tLin\n",
    "date:\n",
    "\t2017 / 11 / 25\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# define functions for non-linear transform\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def logistic(x):\n",
    "    # see the declination of logistic function\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1.0 - ( np.tanh(x) * np.tanh(x) )\n",
    "\n",
    "def logistic_deriv(x):\n",
    "    return logistic(x) * (1 - logistic(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self,layers,activation=\"tanh\"):\n",
    "\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer\n",
    "        Should be at Least two values\n",
    "\n",
    "        :param activation: The activation function to be used. Can be \"Logistic\"\n",
    "        or \"tanh\" function\n",
    "\n",
    "        \"\"\"\n",
    "        if activation ==  'logistic':\n",
    "            self.activation =  logistic\n",
    "            self.activation_deriv = logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_deriv = tanh_deriv\n",
    "        else :\n",
    "        \tprint 'the activation function must be \"tanh\" or \"logistic\".'\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        self.weights = []\n",
    "        \n",
    "\t# append weights for each layer\n",
    "\tfor i in range(1,len(layers)): \t    \n",
    "\t    if(i == (len(layers) - 1)):\n",
    "\t    \tself.weights.append((2*np.random.random((layers[i-1] + 1, layers[i]))-1)*0.25)\n",
    "        else:\n",
    "        \tself.weights.append((2*np.random.random((layers[i-1] + 1, layers[i] + 1))-1)*0.25)\n",
    "            \t\n",
    "    def fit(self,X,y,learningRate=0.2,epochs=10000):\n",
    "    \t\n",
    "    \tX = np.atleast_2d(X)\n",
    "\n",
    "    \tones = np.ones((X.shape[0],1))\n",
    "    \tX = np.concatenate((X,ones),axis=1)\n",
    "    \ty = np.array(y)\n",
    "    \t\n",
    "    \t\n",
    "    \tfor k in range(epochs):\n",
    "    \t    i = np.random.randint(X.shape[0])\n",
    "    \t    a = [X[i]]\n",
    "    \t    \n",
    "    \t    for l in range(len(self.weights)): # going forward network, for each layer\n",
    "    \t        a.append(self.activation(np.dot(a[l],self.weights[l]))) # compute the node for each layer\n",
    "    \t    \n",
    "    \t    error = y[i] - a[-1]\n",
    "    \t    deltas = [error * self.activation_deriv(a[-1])] # For output layer, Err calculation    \n",
    "    \t    \n",
    "    \t    # starting backprobagation\n",
    "    \t    for l in range(len(a)- 2, 0, -1):\n",
    "    \t    \tdeltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l]))\n",
    "    \t    deltas.reverse()\n",
    " \t    for i in range(len(self.weights)):\n",
    " \t        layer = np.atleast_2d(a[i])\n",
    " \t        delta = np.atleast_2d(deltas[i])\n",
    " \t        self.weights[i] += learningRate * layer.T.dot(delta)   \t    \n",
    "       \n",
    "      \n",
    "\n",
    "    def predict(self, x):         \n",
    "        x = np.array(x)         \n",
    "        temp = np.ones(x.shape[0]+1)         \n",
    "        temp[0:-1] = x         \n",
    "        a = temp         \n",
    "        for l in range(0, len(self.weights)):             \n",
    "            a = self.activation(np.dot(a, self.weights[l]))         \n",
    "        return a \n",
    "            \n",
    "            \n",
    "\"\"\"\n",
    "log : \n",
    "\t\n",
    "#2017年 11月 27日 星期日 07:38:46 CST\n",
    "\tmodify the init function\n",
    "\tnow the program could support multiple layers (instead of only 2 layers) \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
